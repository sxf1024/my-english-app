import streamlit as st
import random
import re
import spacy

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¥é…·é‡æ„", layout="wide")

# åŠ è½½ NLP æ¨¡å‹
@st.cache_resource
def load_nlp():
    try:
        return spacy.load("en_core_web_sm")
    except:
        st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ requirements.txt")
        return None

nlp = load_nlp()

st.title("ğŸ§© è‹±è¯­è¿è¯æˆå¥è®­ç»ƒå™¨")
st.markdown("ä¸Šä¼ ä¸€æ®µè‹±æ–‡è¯­æ–™ï¼Œå°†å…¶æ‹†è§£ä¸ºå•è¯å¡ç‰‡ï¼Œç»ƒä¹ ä½ çš„é€ å¥èƒ½åŠ›ï¼")

# --- ç¬¬ä¸€æ­¥ï¼šè¾“å…¥è¯­æ–™ ---
with st.expander("ç¬¬ä¸€æ­¥ï¼šè¾“å…¥/ä¿®æ”¹è¯­æ–™", expanded=True):
    corpus = st.text_area("åœ¨æ­¤ç²˜è´´è‹±æ–‡æ–‡æœ¬:", "The quick brown fox jumps over the lazy dog.", height=150)

# --- ç¬¬äºŒæ­¥ï¼šå¤„ç†è¯­æ–™ ---
def get_sentences(text):
    # ç®€å•çš„å¥å­åˆ†å‰²
    return re.split(r'(?<=[.?!])\s+', text)

def get_tokens(sentence):
    # æ‹†åˆ†å•è¯å’Œæ ‡ç‚¹
    return re.findall(r"[\w']+|[.,!?;]", sentence)

if corpus:
    all_sentences = get_sentences(corpus)
    
    # è®©ç”¨æˆ·é€‰æ‹©è¦ç»ƒä¹ å“ªä¸€å¥
    if len(all_sentences) > 1:
        selected_idx = st.selectbox("é€‰æ‹©è¦ç»ƒä¹ çš„å¥å­:", range(len(all_sentences)), format_func=lambda x: f"å¥å­ {x+1}: {all_sentences[x][:50]}...")
    else:
        selected_idx = 0
    
    target_sentence = all_sentences[selected_idx]
    correct_tokens = get_tokens(target_sentence)

    # --- ç¬¬ä¸‰æ­¥ï¼šå±•ç¤ºæ‰“ä¹±çš„å•è¯ ---
    st.subheader("ç¬¬äºŒæ­¥ï¼šå¼€å§‹æµ‹è¯•")
    
    # éšæœºæ‰“ä¹±ï¼ˆåŸºäºå¥å­å†…å®¹å›ºå®šéšæœºç§å­ï¼Œé˜²æ­¢é¡µé¢ä¸€åˆ·æ–°å•è¯å°±å˜ä½ï¼‰
    shuffled_tokens = correct_tokens[:]
    random.seed(sum(ord(c) for c in target_sentence))
    random.shuffle(shuffled_tokens)

    st.write("æ‰“ä¹±çš„å•è¯å—ï¼š")
    # ç”¨äº®è‰²çš„å°å—å±•ç¤ºå•è¯
    st.write(" ".join([f"`{word}`" for word in shuffled_tokens]))

    # --- ç¬¬å››æ­¥ï¼šç”¨æˆ·è¾“å…¥ä¸æ ¡éªŒ ---
    user_answer = st.text_input("è¯·æŒ‰æ­£ç¡®é¡ºåºè¾“å…¥å®Œæ•´çš„å¥å­ï¼ˆæ³¨æ„æ ‡ç‚¹å’Œç©ºæ ¼ï¼‰:", placeholder="åœ¨è¿™é‡Œè¾“å…¥ç­”æ¡ˆ...")

    if st.button("æäº¤æ£€æŸ¥"):
        user_tokens = get_tokens(user_answer)
        
        # ç»“æœæ˜¾ç¤º
        if user_answer.strip().lower() == target_sentence.strip().lower():
            st.success("ğŸ¯ å®Œå…¨æ­£ç¡®ï¼ä½ å¤ªæ£’äº†ï¼")
        else:
            st.warning("ğŸ§ é¡ºåºæˆ–æ‹¼å†™æœ‰è¯¯ï¼Œå¯¹æ¯”ä¸€ä¸‹ï¼š")
            st.write(f"**ä½ çš„ç­”æ¡ˆ:** {user_answer}")
            st.write(f"**æ ‡å‡†ç­”æ¡ˆ:** {target_sentence}")
        
        # è¯­æ³•è§£æ
        if nlp:
            st.subheader("ğŸ’¡ æ·±åº¦è§£æ")
            doc = nlp(target_sentence)
            cols = st.columns(len(doc))
            for i, token in enumerate(doc):
                with cols[i % 5]: # æ¯è¡Œæ˜¾ç¤º5ä¸ª
                    st.metric(label=token.text, value=token.pos_)
                    st.caption(f"åŠŸèƒ½: {token.dep_}")

st.sidebar.markdown("---")
st.sidebar.info("ä½¿ç”¨è¯´æ˜ï¼š\n1. ç²˜è´´è¯­æ–™\n2. è§‚å¯Ÿæ‰“ä¹±çš„å•è¯\n3. åœ¨è¾“å…¥æ¡†é‡ç»„å¥å­\n4. ç‚¹å‡»æ£€æŸ¥è·å–è§£æ")
